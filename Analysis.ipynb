{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BMA vs Stacking vs Bayesian Mixture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or How I learned to stop worrying and love the Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Model Averaging\n",
    "\n",
    "In BMA, the idea is to create a weighted sum of posterior quantities (Clarke, p 455). For some observed data $y$ and a set of models $\\{M_k\\}_{k=1}^{K}$, then the Bayesian average model for some quantity of interest $\\xi$ is given by the formula:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$p(\\xi \\space | \\space y) = \\sum\\limits_{k=1}^{K} p(\\xi \\space | \\space M_k, y) \\space p(M_k \\space | \\space y)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each model is weighted by its posterior density:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$p(M_k \\space | \\space y) \\propto p(y \\space | \\space M_k) \\space p(M_k)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which depends crucially on the marginal likelihood under that model (see Yao et al 2018)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An ensemble is a way to combine the predictions of different models into a super model. This takes the form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$F_{super}(y \\space | \\space \\theta, \\omega) = \\sum\\limits_{k=1}^{K} \\omega_k f_k(y \\space | \\space \\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\omega_k$ represents the weight associated with model $k$. In the context of stacking, the functions $f$ are densities and the weights are non-negative and sum to 1. Thus the model takes the form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$p(y \\space | \\space \\theta, \\omega) = \\sum\\limits_{k=1}^{K} \\omega_k \\space p_k(y \\space | \\space \\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each $p_k$ is a posterior predictive density."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Mixture Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is similar to an ensemble but instead, the marginal likelihood is a convex combination of data generating processes. This likelihood is multiplied by priors on the weights of the likelihoods and a prior on the joint model parameters to give the following posterior distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$p(\\theta, \\omega \\space | \\space y) \\propto p(\\theta) \\space p(\\omega) \\space \\sum\\limits_{k=1}^{K} \\omega_k \\space p_k(y \\space | \\space \\theta_k)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's the Difference?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayesian Model Averaging computes an average model. Notice that the weights are dependent on the likelihoods. This narrows the scope of the model space being explored. The goal of this is to apportion the responsibility of a model corresponding to its contribution to the prediction in the average model. Compare this to the stacking ensemble, whose weights are only calibrated to achieve some objective (prediction) and are not based on the likelihoods of the submodels. This means that the stacking ensemble has a much broader scope in terms of the model space it covers. Theoretically, if the true model is in the span of the submodels, the ensemble will be able to approximate it by calibrating the weights appropriately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now compare the ensemble to the Bayesian mixture model. Their posterior densities are given below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
